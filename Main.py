import pandas as pd
import re
import numpy as np
import os
import keras
import scipy
import matplotlib.pyplot as plt
from matplotlib import style
import sys
import time
import random
from matplotlib.pyplot import draw
from keras.layers import Dense, GlobalAveragePooling2D, Activation, concatenate, Reshape, Input, Conv2D, Concatenate, BatchNormalization, Add, Dropout
from keras.initializers import VarianceScaling, Ones
from keras.applications import ResNet50
#from keras.preprocessing import image
from PIL import Image
from keras.applications.resnet50 import preprocess_input
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing.image import img_to_array
from keras.models import Model
from keras.optimizers import Adam
import tensorflow as tf
from tensorflow.python.client import device_lib
from keras.preprocessing.image import load_img
from scipy.spatial.transform import Rotation as R
from keras.utils import multi_gpu_model

# print(device_lib.list_local_devices())
def loadImages(data_purpose,scene_info):
    #sequences?
    if (data_purpose == 'train'):
        numImages = scene_info.get('num_images') * len(scene_info.get('train_sequences'))
    elif (data_purpose == 'test'):
        numImages = scene_info.get('num_images') * len(scene_info.get('test_sequences'))
    else:
        sys.exit('data_purpose must be test or train')

    images = np.zeros((numImages, 256, 341, 3))
    xyz = np.zeros((numImages, 3))
    q = np.zeros((numImages, 4))
    image_index = 0

    if (data_purpose == 'train'):
        sequences = scene_info.get('train_sequences')
    else:
        sequences = scene_info.get('test_sequences')

    images_in_seq = scene_info.get('num_images')
    # load the image
    for seq in sequences:
        for i in range(images_in_seq):
            # Load in image
            imageFileName = "./7scenes/{}/seq-{}/frame-{}.color.png".format(scene_info.get('name'),str(seq).zfill(2),str(i).zfill(6))
            img = load_img(imageFileName)
            img = img.resize((341,256),Image.ANTIALIAS)
            images[image_index,:,:,:] = img_to_array(img)

            # Load in pose data
            poseFileName = "./7scenes/{}/seq-{}/frame-{}.pose.txt".format(scene_info.get('name'),str(seq).zfill(2),str(i).zfill(6))
            file_handle = open(poseFileName, 'r')
            # Read in all the lines of your file into a list of lines
            lines_list = file_handle.readlines()
            # Do a double-nested list comprehension to store as a Homogeneous Transform matrix
            homogeneousTransformList = [[float(val) for val in line.split()] for line in lines_list[0:]]
            homogeneousTransform = np.zeros((4,4))

            for j in range(4):
                homogeneousTransform[j,:] = homogeneousTransformList[j]

            # Extract rotation from homogeneous Transform
            r = R.from_dcm(homogeneousTransform[0:3,0:3])
            q[image_index,:] = r.as_quat()
            # Extract xyz from homogeneous Transform
            xyz[image_index,:] = homogeneousTransform[0:3,3]
            file_handle.close()
            image_index += 1

    return images,xyz,q

def center_crop(img, crop_size):
    # Note: image_data_format is 'channel_last'
    assert img.shape[2] == 3
    height, width = img.shape[0], img.shape[1]
    dy, dx = crop_size
    x = (width-dx)//2 + 1
    y = (height-dy)//2 + 1
    return img[y:(y+dy), x:(x+dx), :]

# Following 2 functions copied from https://jkjung-avt.github.io/keras-image-cropping/
def random_crop(img, random_crop_size):
    # Note: image_data_format is 'channel_last'
    assert img.shape[2] == 3
    height, width = img.shape[0], img.shape[1]
    dy, dx = random_crop_size
    x = np.random.randint(0, width - dx + 1)
    y = np.random.randint(0, height - dy + 1)
    return img[y:(y+dy), x:(x+dx), :]

# This function has been modified from source to remove references to yield
def crop_generator(batches, crop_length, isRandom):
    """Take as input a Keras ImageGen (Iterator) and generate random
    crops from the image batches generated by the original iterator.
    """
    batch_crops = np.zeros((batches.shape[0], crop_length, crop_length, 3))
    for i in range(batches.shape[0]):
        if isRandom:
            batch_crops[i] = random_crop(batches[i], (crop_length, crop_length))
        else:
            batch_crops[i] = center_crop(batches[i], (crop_length, crop_length))
    return batch_crops

def insert_layer_nonseq(model, layer_regex, insert_layer_factory,
                        insert_layer_name=None, position='after', special=False, special_layer=None):

    # Auxiliary dictionary to describe the network graph
    network_dict = {'input_layers_of': {}, 'new_output_tensor_of': {}}

    # Set the input layers of each layer
    for layer in model.layers:
        for node in layer._outbound_nodes:
            layer_name = node.outbound_layer.name
            if layer_name not in network_dict['input_layers_of']:
                network_dict['input_layers_of'].update(
                        {layer_name: [layer.name]})
            else:
                network_dict['input_layers_of'][layer_name].append(layer.name)

    # Set the output tensor of the input layer
    network_dict['new_output_tensor_of'].update(
            {model.layers[0].name: model.input})

    # Iterate over all layers after the input
    for layer in model.layers[1:]:

        # Determine input tensors
        layer_input = [network_dict['new_output_tensor_of'][layer_aux]
                for layer_aux in network_dict['input_layers_of'][layer.name]]
        if len(layer_input) == 1:
            layer_input = layer_input[0]

        # Insert layer if name matches the regular expression
        if re.match(layer_regex, layer.name):
            if position == 'replace':
                x = layer_input
            elif position == 'after':
                x = layer(layer_input)
            elif position == 'before':
                pass
            else:
                raise ValueError('position must be: before, after or replace')

            if (special == False):
                new_layer = insert_layer_factory()
                if insert_layer_name:
                    new_layer.name = insert_layer_name
                else:
                    new_layer.name = layer.name
                x = new_layer(x)
            else:
                new_layer = Concatenate([layer_input, special_layer])
                if insert_layer_name:
                    new_layer.name = insert_layer_name
                else:
                    new_layer.name = layer.name
                x = new_layer

            if position == 'before':
                x = layer(x)
        else:
            x = layer(layer_input)

        # Set new output tensor (the original one, or the one of the inserted
        # layer)
        network_dict['new_output_tensor_of'].update({layer.name: x})

    return Model(inputs=model.inputs, outputs=x)

def insert_feedback_loop(model,dropout_rate):
    #########################################################################
    ######    Inserting feedback loop    ####################################
    #########################################################################
    activation_40 = model.get_layer('activation_40').output
    input_xyz_prev = Input(shape=(3,), name='input_xyz_prev')
    input_q_prev = Input(shape=(4,), name='input_q_prev')
    x = concatenate([input_xyz_prev, input_q_prev])
    x = Dense(200704, activation='elu', name='fc4')(x)
    x = Dropout(dropout_rate)(x)
    fc4Reshaped = Reshape((14, 14, -1))(x)
    feedbackLoopInserted = concatenate([x, activation_40])

    ####################################################################
    #    Replacing Res5 components after feedback loop    ##############
    ####################################################################

    ################################
    # Res5a Main Branch ############
    ################################

    #############
    # Res5a A ###
    #############
    # Conv2D ###########################################################################################################
    layer_name = 'res5a_branch2a'
    old_weights = model.get_layer(layer_name).get_weights()[0]
    old_biases = model.get_layer(layer_name).get_weights()[1]
    temp_layer = Conv2D(filters=512, kernel_size=(1, 1), strides=(2, 2), padding='valid', data_format='channels_last', activation='linear',
                kernel_initializer=VarianceScaling(scale=2.0), name='TemporaryLayer')(feedbackLoopInserted)
    temp_model = Model(inputs=[model.input,input_xyz_prev, input_q_prev], outputs=temp_layer)
    new_weights = temp_model.get_layer('TemporaryLayer').get_weights()[0]
    new_weights[:,:,1024:,:] = old_weights;
    x = Conv2D(filters=512, kernel_size=(1, 1), strides=(2, 2), padding='valid', data_format='channels_last', activation='linear',
               kernel_initializer=VarianceScaling(scale=2.0), name=layer_name, weights=[new_weights,old_biases])(feedbackLoopInserted)

    # Batch Normalisation
    layer_name = 'bn5a_branch2a'
    w0 = model.get_layer(layer_name).get_weights()[0]
    w1 = model.get_layer(layer_name).get_weights()[1]
    w2 = model.get_layer(layer_name).get_weights()[2]
    w3 = model.get_layer(layer_name).get_weights()[3]
    x = BatchNormalization(axis=3,name=layer_name,weights=[w0,w1,w2,w3])(x)

    # ELU Activation
    x = Activation('elu',name='activation_41')(x)

    #############
    # Res5a B ###
    #############
    # Conv2D
    layer_name = 'res5a_branch2b'
    W = model.get_layer(layer_name).get_weights()[0]
    b = model.get_layer(layer_name).get_weights()[1]
    x = Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding='same', data_format='channels_last', activation='linear',
               kernel_initializer=VarianceScaling(scale=2.0), name=layer_name, weights=[W,b])(x)

    # Batch Normalisation
    layer_name = 'bn5a_branch2b'
    w0 = model.get_layer(layer_name).get_weights()[0]
    w1 = model.get_layer(layer_name).get_weights()[1]
    w2 = model.get_layer(layer_name).get_weights()[2]
    w3 = model.get_layer(layer_name).get_weights()[3]
    x = BatchNormalization(axis=3, name=layer_name, weights=[w0,w1,w2,w3])(x)

    # ELU Activation
    x = Activation('elu', name='activation_42')(x)

    #############
    # Res5a C ###
    #############
    # Conv2D
    layer_name = 'res5a_branch2c'
    W = model.get_layer(layer_name).get_weights()[0]
    b = model.get_layer(layer_name).get_weights()[1]
    x = Conv2D(filters=2048, kernel_size=(1, 1), strides=(1, 1), padding='valid', data_format='channels_last', activation='linear',
               kernel_initializer=VarianceScaling(scale=2.0), name=layer_name, weights=[W,b])(x)

    # Batch Normalisation
    layer_name = 'bn5a_branch2c'
    w0 = model.get_layer(layer_name).get_weights()[0]
    w1 = model.get_layer(layer_name).get_weights()[1]
    w2 = model.get_layer(layer_name).get_weights()[2]
    w3 = model.get_layer(layer_name).get_weights()[3]
    x = BatchNormalization(axis=3, name=layer_name, weights=[w0, w1, w2, w3])(x)

    ################################
    # Res5a Skip Branch ############
    ################################
    # Conv2D ###########################################################################################################
    layer_name = 'res5a_branch1'
    old_weights = model.get_layer(layer_name).get_weights()[0]
    old_biases = model.get_layer(layer_name).get_weights()[1]
    temp_layer = Conv2D(filters=2048, kernel_size=(1, 1), strides=(2, 2), padding='valid', data_format='channels_last', activation='linear',
               kernel_initializer=VarianceScaling(scale=2.0), name='TemporaryLayer2')(feedbackLoopInserted)
    temp_model = Model(inputs=[model.input, input_xyz_prev, input_q_prev], outputs=temp_layer)
    new_weights = temp_model.get_layer('TemporaryLayer2').get_weights()[0]
    new_weights[:, :, 1024:, :] = old_weights;
    y = Conv2D(filters=2048, kernel_size=(1, 1), strides=(2, 2), padding='valid', data_format='channels_last', activation='linear',
               kernel_initializer=VarianceScaling(scale=2.0), name=layer_name, weights=[new_weights,old_biases])(feedbackLoopInserted)

    # Batch Normalisation
    layer_name = 'bn5a_branch1'
    w0 = model.get_layer(layer_name).get_weights()[0]
    w1 = model.get_layer(layer_name).get_weights()[1]
    w2 = model.get_layer(layer_name).get_weights()[2]
    w3 = model.get_layer(layer_name).get_weights()[3]
    y = BatchNormalization(axis=3, name=layer_name, weights=[w0, w1, w2, w3])(y)

    ################################
    # Res5b Main Branch ############
    ################################
    x = Add(name='add_14')([x,y])
    y = Activation('elu', name='activation_43')(x)

    #############
    # Res5b A ###
    #############
    # Conv2D
    layer_name = 'res5b_branch2a'
    W = model.get_layer(layer_name).get_weights()[0]
    b = model.get_layer(layer_name).get_weights()[1]
    x = Conv2D(filters=512, kernel_size=(1, 1), strides=(1, 1), padding='valid', data_format='channels_last',
               activation='linear', kernel_initializer=VarianceScaling(scale=2.0), name=layer_name, weights=[W,b])(y)

    # Batch Normalisation
    layer_name = 'bn5b_branch2a'
    w0 = model.get_layer(layer_name).get_weights()[0]
    w1 = model.get_layer(layer_name).get_weights()[1]
    w2 = model.get_layer(layer_name).get_weights()[2]
    w3 = model.get_layer(layer_name).get_weights()[3]
    x = BatchNormalization(axis=3, name=layer_name, weights=[w0, w1, w2, w3])(x)

    # ELU Activation
    x = Activation('elu', name='activation_44')(x)

    #############
    # Res5b B ###
    #############
    # Conv2D
    layer_name = 'res5b_branch2b'
    W = model.get_layer(layer_name).get_weights()[0]
    b = model.get_layer(layer_name).get_weights()[1]
    x = Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding='same', data_format='channels_last',
               activation='linear', kernel_initializer=VarianceScaling(scale=2.0), name=layer_name, weights=[W,b])(x)

    # Batch Normalisation
    layer_name = 'bn5b_branch2b'
    w0 = model.get_layer(layer_name).get_weights()[0]
    w1 = model.get_layer(layer_name).get_weights()[1]
    w2 = model.get_layer(layer_name).get_weights()[2]
    w3 = model.get_layer(layer_name).get_weights()[3]
    x = BatchNormalization(axis=3, name=layer_name, weights=[w0, w1, w2, w3])(x)

    # ELU Activation
    x = Activation('elu', name='activation_45')(x)

    #############
    # Res5b C ###
    #############
    # Conv2D
    layer_name = 'res5b_branch2c'
    W = model.get_layer(layer_name).get_weights()[0]
    b = model.get_layer(layer_name).get_weights()[1]
    x = Conv2D(filters=2048, kernel_size=(1, 1), strides=(1, 1), padding='valid', data_format='channels_last',
               activation='linear', kernel_initializer=VarianceScaling(scale=2.0), name=layer_name, weights=[W,b])(x)

    # Batch Normalisation
    layer_name = 'bn5b_branch2c'
    w0 = model.get_layer(layer_name).get_weights()[0]
    w1 = model.get_layer(layer_name).get_weights()[1]
    w2 = model.get_layer(layer_name).get_weights()[2]
    w3 = model.get_layer(layer_name).get_weights()[3]
    x = BatchNormalization(axis=3, name=layer_name, weights=[w0, w1, w2, w3])(x)

    ################################
    # Res5c Main Branch ############
    ################################
    x = Add(name='add_15')([x, y])
    y = Activation('elu', name='activation_46')(x)

    #############
    # Res5c A ###
    #############
    # Conv2D
    layer_name = 'res5c_branch2a'
    W = model.get_layer(layer_name).get_weights()[0]
    b = model.get_layer(layer_name).get_weights()[1]
    x = Conv2D(filters=512, kernel_size=(1, 1), strides=(1, 1), padding='valid', data_format='channels_last',
               activation='linear', kernel_initializer=VarianceScaling(scale=2.0), name=layer_name, weights=[W,b])(y)

    # Batch Normalisation
    layer_name = 'bn5c_branch2a'
    w0 = model.get_layer(layer_name).get_weights()[0]
    w1 = model.get_layer(layer_name).get_weights()[1]
    w2 = model.get_layer(layer_name).get_weights()[2]
    w3 = model.get_layer(layer_name).get_weights()[3]
    x = BatchNormalization(axis=3, name=layer_name, weights=[w0, w1, w2, w3])(x)

    # ELU Activation
    x = Activation('elu', name='activation_47')(x)

    #############
    # Res5c B ###
    #############
    # Conv2D
    layer_name = 'res5c_branch2b'
    W = model.get_layer(layer_name).get_weights()[0]
    b = model.get_layer(layer_name).get_weights()[1]
    x = Conv2D(filters=512, kernel_size=(3, 3), strides=(1, 1), padding='same', data_format='channels_last',
               activation='linear', kernel_initializer=VarianceScaling(scale=2.0), name=layer_name, weights=[W,b])(x)

    # Batch Normalisation
    layer_name = 'bn5c_branch2b'
    w0 = model.get_layer(layer_name).get_weights()[0]
    w1 = model.get_layer(layer_name).get_weights()[1]
    w2 = model.get_layer(layer_name).get_weights()[2]
    w3 = model.get_layer(layer_name).get_weights()[3]
    x = BatchNormalization(axis=3, name=layer_name, weights=[w0, w1, w2, w3])(x)

    # ELU Activation
    x = Activation('elu', name='activation_48')(x)

    #############
    # Res5c C ###
    #############
    # Conv2D
    layer_name = 'res5c_branch2c'
    W = model.get_layer(layer_name).get_weights()[0]
    b = model.get_layer(layer_name).get_weights()[1]
    x = Conv2D(filters=2048, kernel_size=(1, 1), strides=(1, 1), padding='valid', data_format='channels_last',
               activation='linear', kernel_initializer=VarianceScaling(scale=2.0), name=layer_name, weights=[W,b])(x)

    # Batch Normalisation
    layer_name = 'bn5c_branch2c'
    w0 = model.get_layer(layer_name).get_weights()[0]
    w1 = model.get_layer(layer_name).get_weights()[1]
    w2 = model.get_layer(layer_name).get_weights()[2]
    w3 = model.get_layer(layer_name).get_weights()[3]
    x = BatchNormalization(axis=3, name=layer_name, weights=[w0, w1, w2, w3])(x)

    ##################
    # End of Res5c ###
    ##################
    x = Add(name='add_16')([x, y])
    x = Activation('elu', name='activation_49')(x)

    return Model(inputs=[model.input,input_xyz_prev, input_q_prev], outputs=x)

# Replace all ReLUs with ELUs
def dropout_layer_factory():
    return Activation('elu')

def additional_final_layers(model):
    x = base_model.output
    x = GlobalAveragePooling2D()(x)  # **** Assuming 2D, with no arguments required
    #x = Dropout(dropout_rate)(x)
    x = Dense(1024, activation='relu', name='fc1')(x)  # **** Assuming relu
    #x = Dropout(dropout_rate)(x)
    xyz = Dense(3, name='xyz')(x)  # **** Assuming softmax is the correct activation here
    q = Dense(4, name='q')(x)  # **** Assuming softmax (rho/theta/phi) and quaternians

    return Model(inputs=model.inputs, outputs=[xyz,q])

def Train_epoch(scene_info, datagen, model, quickTrain):
    xyz_error_sum = 0
    q_error_sum = 0
    num_scenes = 0
    for scene in scene_info:
        x_train, y_xyz_train, y_q_train = loadImages('train', scene)
        datagen.fit(x_train)
        for j in range(len(x_train)):
            x_train[j, :, :, :] = datagen.standardize(x_train[j, :, :, :])
        x_train = crop_generator(x_train, 224, isRandom=True)
        history = model.fit(x=x_train, y={'xyz': y_xyz_train, 'q': y_q_train}, batch_size=32, verbose=0, shuffle=True)
        xyz_error_sum += history.history["xyz_mean_absolute_error"][0]
        q_error_sum += history.history["q_mean_absolute_error"][0]
        num_scenes += 1
        if (quickTrain):
            break
    return model, xyz_error_sum/num_scenes, q_error_sum/num_scenes

def Test_epoch(scene_info, datagen, model, quickTest):
    xyz_error_sum = 0
    q_error_sum = 0
    num_scenes = 0
    for scene in scene_info:
        x_test, y_xyz_test, y_q_test = loadImages('test', scene)
        datagen.fit(x_test)
        for i in range(len(x_test)):
            x_test[i, :, :, :] = datagen.standardize(x_test[i, :, :, :])
        x_test = crop_generator(x_test, 224, isRandom=False)
        results = model.evaluate(x=x_test, y={'xyz': y_xyz_test, 'q': y_q_test}, verbose=0)
        print(results)
        xyz_error_sum += results[3]
        q_error_sum += results[4]
        num_scenes += 1
        if (quickTest):
            break
    return xyz_error_sum/num_scenes, q_error_sum/num_scenes

def update_graph(xs,xyz,q):
    plt.clf()
    plt.plot(xs, xyz, label='xyz error')
    plt.plot(xs, q, label='q error')
    plt.legend(loc='upper right')
    plt.ylabel('Test Mean Absolute Error')
    plt.xlabel('Epoch')
    plt.draw()
    plt.pause(0.001)

########################################################################################################################
########################################################################################################################
####################################                                               #####################################
####################################  #######  #######  #######  ######   #######  #####################################
####################################  #           #     #     #  #     #     #     #####################################
####################################  #######     #     #######  ######      #     #####################################
####################################        #     #     #     #  #    #      #     #####################################
####################################  #######     #     #     #  #     #     #     #####################################
####################################                                               #####################################
########################################################################################################################
########################################################################################################################

with tf.device('/device:GPU:0'):
    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))
    print("ResNet50 model loaded...")
    #for layer in base_model.layers[:143]:
    #    layer.trainable = False


    #dropout_rate = 0.2

    #base_model = insert_layer_nonseq(model=base_model, layer_regex='.*activation.*', insert_layer_factory=dropout_layer_factory, position='replace')

    #base_model = insert_feedback_loop(base_model,dropout_rate)

    base_model = additional_final_layers(base_model) #dropout rate removed

    #global_pose_network = multi_gpu_model(base_model,gpus=2)
    global_pose_network = base_model

    global_pose_network.compile(optimizer=Adam(lr=1e-4,epsilon=1e-10),loss='mean_squared_error', metrics=['mean_absolute_error'])
    #global_pose_network.summary()

    scene_info = [{'name':'chess', 'train_sequences': [1, 2, 4, 6], 'test_sequences': [3, 5], 'num_images': 1000},
                  {'name':'fire', 'train_sequences': [1, 2], 'test_sequences': [3, 4], 'num_images': 1000},
                  {'name':'heads', 'train_sequences': [2], 'test_sequences': [1], 'num_images': 1000},
                  {'name':'office', 'train_sequences': [1, 3, 4, 5, 8, 10], 'test_sequences': [2, 6, 7, 9], 'num_images': 1000},
                  {'name':'pumpkin', 'train_sequences': [2, 3, 6, 8], 'test_sequences': [1, 7], 'num_images': 1000},
                  {'name':'redkitchen', 'train_sequences': [1, 2, 5, 7, 8, 11, 13], 'test_sequences': [3, 4, 6, 12, 14], 'num_images': 1000},
                  {'name':'stairs', 'train_sequences': [2, 3, 5, 6], 'test_sequences': [1, 4], 'num_images': 500}]

    ######################################################################
    ###############  Training  ###########################################
    ######################################################################
    print('*****************************')
    print('***** STARTING TRAINING *****')
    print('*****************************')
    style.use('fast')
    datagen = ImageDataGenerator(featurewise_center=False)
    xyz_avg_error = []
    q_avg_error = []
    xs = []
    #plt.ion()
    #plt.show()
    file1 = open("Results.txt", "w")
    # Base-line accuracy
    test_xyz_error,test_q_error = Test_epoch(scene_info=scene_info, datagen=datagen, model=global_pose_network, quickTest=False)
    file1.write("0,,%s,,%s\n" % (test_xyz_error,test_q_error))
    file1.close()
    xs.append(0)
    xyz_avg_error.append(test_xyz_error)
    q_avg_error.append(test_q_error)
    #update_graph(xs,xyz_avg_error,q_avg_error)



    # Train many epochs
    epoch_max = 300
    epochs_per_result = 5
    result_index = epochs_per_result
    for epoch in range(1,epoch_max+1):
        print('Epoch: ',epoch,'/',epoch_max,sep='')
        global_pose_network, train_xyz_error, train_q_error = Train_epoch(scene_info=scene_info, datagen=datagen, model=global_pose_network, quickTrain=False)
        #time.sleep(1)
        if ((epoch % epochs_per_result) == 0):
            test_xyz_error,test_q_error = Test_epoch(scene_info=scene_info, datagen=datagen, model=global_pose_network, quickTest=True)
            print("Testing: [test_xyz_error,test_q_error] = [",test_xyz_error,", ",test_q_error,"]",sep='')
            file1 = open("Results.txt", "a")
            file1.write("%s,%s,%s,%s,%s\n" % (result_index, train_xyz_error,test_xyz_error, train_q_error,test_q_error))
            file1.close()
            xs.append(result_index)
            xyz_avg_error.append(test_xyz_error)
            q_avg_error.append(test_q_error)
            result_index += epochs_per_result
            #update_graph(xs,xyz_avg_error, q_avg_error)





    print("Finished Successfully")
    #plt.show()